{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic Regression Coding Assignment Blank.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jIQPwdlNoW-8"},"source":["# Non-linear Decision Boundaries - Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"PWHBL988K22y"},"source":["Throughout this coding homework assignment, you will work with a well-known dataset in machine learning field: Wisconsin breast cancer dataset from UCI Machine Learning Repository.\n","\n","First you will be guided to transit from linear regression on continuous observations to binary label variables. You will see why linear regression is not the best model to use when you try to predict a binary class label.\n","\n","Second, you will review fundamental concepts of logistic regression you have seen in the course note and lecture. But the emphasis at this time is to visualize properties of sigmoid function, and you'll have a chance to implement loss function and decision rule etc of logistic regression you have seen in theory.\n","\n","Third, you will visualize a simple logistic regression model using only one numerical feature with an intercept. You cannot find optimal weight vector at this point, but the pedagogical purpose of this part is to take advantage of all functions you implemented in the second part. So you can fully understand how and why logistic regression works, without being introduced to sklearn library.\n","\n","Lastly, you will use all features provided in the breast cancer dataset, and build a logistic regression model using sklearn library. You will get hands on experience in data cleaning, EDA, model training, and evaluation metrics. This will give you enough exposure to functions of sklearn library related to binary classification problem."]},{"cell_type":"code","metadata":{"id":"Nz3RKErHXCwd"},"source":["# import necessary library and setup\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZXNiOiVf-BO"},"source":["## Introduction: From Linear Regresson to Binary Classification"]},{"cell_type":"markdown","metadata":{"id":"dksVY7XIQ0m3"},"source":["Recal a regression problem in machine learning is to construct a mapping function, which takes in argument as feature vector of a single data point in feature space, and ouputs a predicted value based on the input. The purpose of this type of problem is to approximate outputs of this function as close as possible to underlying true observed value in response variable. So far we have only seen linear models. A model is a linear model if it is a linear combination of features in the feature space. You should remember that there is always a closed form solution to the linear model given features and observations.\n","\n","We talk about classification problem when you are introduced to support vector machine (SVM) in classification problem. Now in order to output a prediction of a binary class label, we cannot use traditional linear regression since it outputs continuous predictions. SVM produces a linear decision boundary in feature space, but it works well only when data is linearly separable.\n","\n","Now, we are curious about the case where data is not linearly separable. Extending from linear models to non-linear decision functions might make our life easier. We further want to suggest a probablistic interpretation of classification problem. Given binary labels 0 and 1, we can simply interpret our prediction as a probability that a given data point takes label value of 1. "]},{"cell_type":"markdown","metadata":{"id":"2mRhXSo0SdQ5"},"source":["First, we want to [load Wisconsin breast cancer dataset from sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html). We start with a model with only one single feature \"mean radius\", which is the first column. You will observe how linear regression model performs by visualizing its decision boundary. With this intuition in mind, you will understand why we want to introduce logistic regression today."]},{"cell_type":"code","metadata":{"id":"-fTBHQTSODjh"},"source":["# import breast cancer dataset from sklearn library\n","from sklearn.datasets import load_breast_cancer\n","dataset = load_breast_cancer()\n","X_simple = pd.DataFrame(dataset.data, columns=dataset.feature_names)[['mean radius']]\n","y_simple = dataset.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rXhvLZUOyMM"},"source":["# train test split\n","from sklearn.model_selection import train_test_split\n","\n","X_simple_train, X_simple_test, y_simple_train, y_simple_test = train_test_split(X_simple, y_simple, test_size = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8mvLfkvTNWZ"},"source":["X_simple.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aelGnLUXS9LD"},"source":["**Question: Visualize training set in a 2-D plane. Plot feature `mean radius` on x-axis and response variable `diagnosis` on y-axis.** "]},{"cell_type":"code","metadata":{"id":"sieXRTriPful"},"source":["# visualize training set\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use scatterplot to visualize training set\n","\n","plt.ylabel('diagnosis')\n","plt.title('Mean radius vs Breast Cancer Diagnosis');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INjOxdH_d2Rs"},"source":["**Question: Comment on whether we are given a linearly separable data. Can you guess what type of model linear regression will output? What will be the decision boundary if we use support vector machine?**"]},{"cell_type":"markdown","metadata":{"id":"gC9bFbz6RQpq"},"source":["**Question: Implement the following functions to solve for and visualize ordinay least square linear regression without regularization. `ordinary_least_square` finds close form solution to optimal weight vector in linear regression. `ols_predict` uses weight vector to predict a new test point. `mse` compute mean squared error of linear regression model on a given dataset.**"]},{"cell_type":"code","metadata":{"id":"ZkhtMjbKQqPc"},"source":["# TODO: solve ordinary least square linear regression solution\n","def ordinary_least_square(X, y):\n","  return ...\n","\n","# TODO: linear regression prediction\n","def ols_predict(X, w):\n","  return ...\n","\n","# TODO: compute mean squared error\n","def mse(y, y_pred):\n","  return ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAEN56kARr9d"},"source":["Now you want to fit a linear regression model to this training set, only using \"mean radius\" feature and an extra intercept term. That is, our linear regression model should be in the form:\n","\n","$$ \\hat{y} = \\hat{w}x + \\hat{b} $$.\n","\n","Use what you learned from previous week about linear regression to find close form solution to $w$ and $b$."]},{"cell_type":"markdown","metadata":{"id":"OyQk2Jm-gXYo"},"source":["**Question: First append an intercept term to feature dataframe. Then use your defined function to solve for optimal least square solution**"]},{"cell_type":"code","metadata":{"id":"GF91airUPkLM"},"source":["# TODO: append intercept column to dataframe\n","X_simple_train.loc[:, 'intercept'] = ...\n","X_simple_test.loc[:, 'intercept'] = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLcG7OWQTrgp"},"source":["# TODO: solve for optimal weight vector in simple linear regression model\n","# here we know optimal weight vector only has two terms\n","vec_ols = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7B4mL8IlWVHG"},"source":["**Question: Compute predictions of linear regression model on training set. Evaluate and report MSE on training set. Visualize predictions of linear regression model, and compare with true binary label values.**"]},{"cell_type":"code","metadata":{"id":"gB10GojtUD3a"},"source":["# TODO: predict label values on training set\n","y_pred_simple_train = ...\n","\n","# TODO: append prediction column to X_simple_train\n","X_simple_train...\n","\n","# TODO: compute MSE of linear regression model on training set\n","ols_simple_train_mse = ...\n","\n","print(\"MSE of linear regression model with one feature and intercept term on training set:\", ols_simple_train_mse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hlz3Pz33UeRG"},"source":["# visualize predictions of linear regression model on training set \n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use scatterplot to visualize true label value of diagnosis, same as what you did in the previous part\n","#       use lineplot to visulize predicted value given by linear regression model\n","\n","plt.title('Linear Regression Prediction on Training Set: One feature with an Intercept Term');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ny6l2hx9dyej"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUrZuxkn-hUs"},"source":["Use the generic rule, classify the data point as label 1 if ordinary least square regression returns a predicted value greater than or equal to 1/2, and label value 0 otherwise.\n","\n","**Question: Add ordinary least square predicted labels as a column to `X_simple_train` as `ols_pred_label`. Report training accuracy of linear regression.**"]},{"cell_type":"code","metadata":{"id":"Hca2wtpUeEy2"},"source":["# TODO: Compute label value predicted by ols and add the corresponding column"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHTtqaksd_8K"},"source":["# TODO: Compute training accuracy of ordinary least square model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZbSAmN6wXi_J"},"source":["**Question: Perform the same procedure above on test set. What similarity do you observe? What do you want to say about linear regression model on classification problem?**"]},{"cell_type":"code","metadata":{"id":"FSdQW9X3Xwc6"},"source":["# TODO: predict label values on test set\n","y_pred_simple_test = ...\n","\n","# TODO: append prediction column to X_simple_train\n","X_simple_test...\n","\n","# TODO: compute MSE of linear regression model on training set\n","ols_simple_test_mse = ...\n","\n","print(\"MSE of linear regression model with one feature and intercept term on test set:\", ols_simple_test_mse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3zdOMMd0X5H9"},"source":["# visualize predictions of linear regression model on test set \n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use scatterplot to visualize true label value of diagnosis, same as what you did in the previous part\n","#       use lineplot to visulize predicted value given by linear regression model\n","\n","plt.title('Linear Regression Prediction on Test Set: One feature with an Intercept Term');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jy4bnQxKflB5"},"source":["## Theories of Logistic Regression: Fundamental Concepts"]},{"cell_type":"markdown","metadata":{"id":"XmaiZLWyRBGE"},"source":["You shold observe from previous example, that linear regression model on classification problem doesn't perform well as expected. Linear regression model predicts a continuous numerical value from given feature space, but it's difficult for us to transform that numerical value into an 0-1 binary label. These values are not strictly between 0 and 1, so we cannot make this transformation without adding other heuristics. As you should see from previous weeks, ordinary least square regression model can be sensitive to outliers, and is thus unreliable.\n","\n","This is where logistic regression models plays an important role in classification. You'll start examine binary classification problem in this assignment, and we'll introduce its extension to multiclass classification problem in the course note."]},{"cell_type":"markdown","metadata":{"id":"fpKuwuDJSHR-"},"source":["### Definition\n","\n","In a binary classification problem, the logistic regression model predicts the **probability** that the **binary** response variable $Y$ takes the value of 1 given the features $x \\in R^d$:\n","\n","$$ P(Y = 1|x) = f_\\theta(x) = \\frac {1} {1 + \\exp(-\\sum_{k=1} ^{d} {\\theta_k x_k})}$$\n","\n","Similar to linear regression model, **parameters** we want to learn in logistic regression model is weight vector $\\theta \\in R^d$. You should observe that in prediction, we still have predicted probability as a **function** of $\\theta^T x$. So logistic regression can usually be interpreted as an extention of linear model, called **generalized linear model**."]},{"cell_type":"markdown","metadata":{"id":"2H1IG9isUOFQ"},"source":["### Logistic Activation Function\n","\n","Logistic regression model derives from **logisitic function**, called **sigmoid**:\n","$$ s(t) = \\frac {1} {1 + e^{-t}} $$\n","\n","You should observe that sigmoid function takes in a feature vector in $R^d$ and outputs a probability between 0 and 1. This is how logistic regression always work as intended in binary classification problem. Sigmoid function is so commonly used in ML field, as you can see more of it's applications in the following week when we talk about neural network.\n","\n","**Question: Let's look at properties of sigmoid function. Implement sigmoid function in python.**"]},{"cell_type":"code","metadata":{"id":"4PILvvupXMal"},"source":["# TODO: implement sigmoid function\n","def sigmoid(t):\n","  return ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4083-S-mXt5j"},"source":["\n","\n","*   Domain of $s(t)$: $-\\infty < t < \\infty$\n","*   Range of $s(t)$: $0 < s(t) < 1$\n","*   Threshold Value: $s(0) = 0.5$\n","*   Reflection and Symmetry of $s(t)$: \n","$$ 1 - s(t) = 1 - \\frac {1} {1 + e^{-t}} = \\frac {e^{-t}} {1 + e^{-t}} = s(-t) $$\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"au-onbwOZwyk"},"source":["# plot shape of logistic function\n","t = np.linspace(-8, 8, 100)\n","plt.plot(t, sigmoid(t), lw=2) \n","plt.xticks(range(-8, 9))\n","plt.yticks(np.arange(0, 1.1, 0.25))\n","plt.xlabel('$t$')\n","plt.ylabel('$s(t)$')\n","plt.title('logistic function s(t)');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6oARxDNapDU"},"source":["# plot symmetry of logistic function\n","t = np.linspace(-4, 4, 100)\n","plt.plot(t, sigmoid(t), lw=2) \n","plt.xticks(range(-4, 5))\n","plt.yticks(np.arange(0, 1.1, 0.25))\n","plt.plot([1, 1], [sigmoid(1), 1], lw=2, color='red')\n","plt.plot([-1, -1], [0, sigmoid(-1)], lw=2, color='green')\n","plt.xlabel('$t$')\n","plt.ylabel('$s(t)$')\n","plt.title('logistic function s(t)');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-nP4EMwbvch"},"source":["\n","*   Inverse of $s(t)$: \n","$$ s(t) = z = \\frac {1} {1 + e^{-t}}, 0 < z < 1 $$\n","$$ e^{-t} = \\frac {1-z} {z}$$\n","$$ t = -\\log(\\frac {1-z} {z}) = \\log(\\frac {z} {1-z}), 0 < z < 1 $$\n","*   Derivative of $s(t)$ using chain rule:\n","$$ s'(t) = -\\frac {-e^{t}} {(1 + e^{-t})^2} = s(t)(1 - s(t)) $$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"62LsCjmwjXhQ"},"source":["### Decision Rule\n","\n","Let's look at how logistic activation function exactly solves binary classification problem. Recall you saw in course note that output of sigmoid function is interpreted as probability that data point takes label 1. Given $s(0) = \\frac {1}{2}$, intuitively we can use the sign of sigmoid function as decision rule to classify 0 or 1. This is a generalization to decision rule when we try to predict label of a test point, and this is indeed how `sklearn` implements logistic regression model:\n","*   **If $s(\\hat {\\theta}^Tx) \\ge \\frac{1}{2}$, predict this data point as class 1.**\n","*   **If $s(\\hat {\\theta}^Tx) < \\frac{1}{2}$, predict this data point as class 0.**\n","\n","You can think of this decision rule as using a **\"threshold\"** value of $\\frac {1}{2}$ based on sigmoid function. Then this threshold value is a hyperparameter in logistic regression model. As always, you can use cross validation to tune this hyperparameter with custom values. It's not always true that $\\frac{1}{2}$ is the best threshold to implement the decision rule on unseen test data.\n","\n","**Question: Now implement decision function of our naive classifier. It takes input of a feature vector $x \\in R^d$, a weight vector $\\theta \\in R^d$, and ouputs a prediction of binary label 0 or 1 for this data point.**"]},{"cell_type":"code","metadata":{"id":"FziqxBUY71Ly"},"source":["# TODO: implement decision funcition of logistic regression model\n","def decision_rule(x, theta):\n","  return ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AB2X46zB3FPc"},"source":["### Loss Function\n","\n","Recall in linear regression, we use mean squared error (MSE) as loss function. We find optimal weight vector by minimizing the squared loss on training set.\n","\n","However, MSE doesn't work as intended in classification problem. In classification problem, each prediction happens in terms of the probability of a data point being label 1. For example, a test point has observed true label of 1, and our classifier predicts its probability of 1 as 0.6. Based on decision rule, the classifier will report a prediction of 1. MSE of this data point will report 0. However, we know the actual predicted probability is 0.6, still far away from true probability 1. So we are still far from minimizing the loss, and we need to define a new loss function that can account for this 0.4 gap."]},{"cell_type":"markdown","metadata":{"id":"n0L7vhJj5Bvp"},"source":["This is how **negative log-likelihood function** as loss in classification comes from. It is also called **cross entropy** loss.\n","\n","Given training points $(x_i, y_i), i = 1...n$, cross entropy loss is defined as\n","\n","$$ L(\\theta) = - \\sum_{i=1}^{n} {y_i\\log(f_\\theta(x_i)) + (1-y_i)\\log(1 - f_\\theta(x_i))} $$\n","\n","Unlike squared loss, cross entropy loss is not a convex function, so there's no close form solution to it. Usually you need to use gradient descent to find a global optimum with respect to data."]},{"cell_type":"markdown","metadata":{"id":"oOEuZbQLBFye"},"source":["**Question: Implement cross entropy loss function below as `cross_entropy_loss`. It should take in arguments `labels`, which are true labels, and `pred_probs`, which are predicted probabilities, and ouput cross entropy loss of this classifier on this dataset.**"]},{"cell_type":"code","metadata":{"id":"TwK7whgfBasA"},"source":["# TODO: implement cross entropy loss function\n","def cross_entropy_loss(labels, pred_probs):\n","  return ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjfEkJKwYN3S"},"source":["## A Closer Look into Logistic Regression: One Feature with an Intercept Term"]},{"cell_type":"markdown","metadata":{"id":"aB1SIiNaYh1T"},"source":["Now you will redo what you did in first part, predicting binary class label of Wisconsin Breast Cancer Dataset with only one feature \"mean radius\" and an extra intercept term. But now you want to choose a logistic regression model."]},{"cell_type":"markdown","metadata":{"id":"5crvg46SdqWw"},"source":["Recall that logistic regression model can be interpreted as a generalized linear regression model. Feature space is first linearly parametrized by a weight vector $t = wx + b $, and then processed by sigmoid function $s(t)$ that turns a numerical value to a 0-1 probability. Since logistic weights cannot be minimized in close form formula, you want to spend sometime to explore how to improve prediction accuracy manually.\n","\n","**Question: You can start with an arbitrary weight vector, corresponding to one feature \"mean radius\" and an extra intercept term. Assign this value you choose to `vec_lr`, it should be 2 dimensional.** \n","\n","A good hint to start is notice visualization of training and test points has reverse shape of signmoid function we plotted in previous part. What does this imply about sign of $w$ in our logistic regression model?"]},{"cell_type":"code","metadata":{"id":"FJ-XRoN9hRZX"},"source":["# TODO: select weight vector parameter\n","# you may want to change parameter values in vec_lr\n","vec_lr = np.array([..., ...]).reshape(2, 1)\n","vec_lr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9F4B9LIUi6sx"},"source":["# visualize structure of `X_simple_train`\n","X_simple_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y38wIqK-lYIR"},"source":["**Question: Use the weight vector `vec_lr` you choose, and feed it into your logistic regression model. Use function you defined in the previous part, to compute predicted probability of each training sample has label value 1.**"]},{"cell_type":"code","metadata":{"id":"sxYWCSV7i-Np"},"source":["# TODO: predicted probability of logistic regression model on training set\n","# HINT: `y_pred_probs_simple_train` should be a column vector in same length as number of training samples\n","y_pred_probs_simple_train = ...\n","\n","# append logistic regression predicted probabilities to X_simple_train\n","X_simple_train['lr_pred_probs'] = y_pred_probs_simple_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqKB6Cr_xfFQ"},"source":["X_simple_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DU8BIOHymIBM"},"source":["**Question: Compute MSE and cross entropy loss of logistic regression model on training set.**"]},{"cell_type":"code","metadata":{"id":"6uV8SUFolI9H"},"source":["# TODO: compute mse\n","lr_simple_train_mse = ...\n","print(\"MSE of linear regression model with one feature and intercept term on training set:\", lr_simple_train_mse)\n","\n","# TODO: compute cross entropy loss\n","# Solution\n","lr_simple_train_entropy = cross_entropy_loss(y_simple_train, y_pred_probs_simple_train[0])\n","print(\"Cross entropy loss of linear regression model with one feature and intercept term on training set:\", lr_simple_train_entropy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4aAgP1WmVC2"},"source":["**Question: Now we want to visualize decision boundary of logistic regression model. As before in linear regression model, use [scatterplot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) for true label and feature values, and use [lineplot](https://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot) for predicted probabilities given by logistic regression model.**"]},{"cell_type":"code","metadata":{"id":"imPSxgb6lePO"},"source":["# visualize logistic regression predictions\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use scatterplot to visualize true label value of diagnosis, same as what you did in the previous part\n","#       use lineplot to visulize predicted value given by linear regression model\n","\n","plt.title('Logistic Regression Prediction on Training Set: One feature with an Intercept Term');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zz0csX8K9fM5"},"source":["We want to use general decision rule: predict label 1 if sigmoid function gives probability is greater than 1/2, and label 0 otherwise.\n","\n","**Question: add predicted label value as a column `lr_pred_label` to `X_simple_train` and report training accuracy.**"]},{"cell_type":"code","metadata":{"id":"5L5h4GchehbX"},"source":["# TODO: Add predicted label value column of logistic regression model to X_simple_train\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UlnBW-keie-"},"source":["# TODO: report training accuracy of logistic regression model using general threshold of 1/2\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlTieDMKUnQg"},"source":["Now from visualization, you can think of logistic regression model as approximating some transformation of sigmoid function as close to distribution of true observed labels in the training set. The way we use gradient descent to minimize cross entropy loss is to consistently changing shape of sigmoid function of logistic regression model, and find an optimal solution.\n","\n","**Question: To complete fitting this simple logistic regression model, you want to evaluate its predicted probability on test set. Complete the same process as the above on test set, and report your observations.**"]},{"cell_type":"code","metadata":{"id":"ZEng2jURbdKr"},"source":["# TODO: predicted probability of logistic regression model on test set\n","y_pred_probs_simple_test = ...\n","\n","# append logistic regression predicted probabilities to X_simple_test\n","X_simple_test['lr_pred_probs'] = y_pred_probs_simple_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XdMUr_LtbnRc"},"source":["X_simple_test.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwXt-Ch4brWL"},"source":["# TODO: compute mse\n","lr_simple_test_mse = ...\n","print(\"MSE of linear regression model with one feature and intercept term on test set:\", lr_simple_test_mse)\n","\n","# TODO: compute cross entropy loss\n","lr_simple_test_entropy = ...\n","print(\"Cross entropy loss of linear regression model with one feature and intercept term on test set:\", lr_simple_test_entropy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcBdXIyQb3cz"},"source":["# visualize logistic regression predictions\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use scatterplot to visualize true label value of diagnosis, same as what you did in the previous part\n","#       use lineplot to visulize predicted value given by linear regression model\n","\n","plt.title('Logistic Regression Prediction on Test Set: One feature with an Intercept Term');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UAJzje5ue0Ik"},"source":["**Question: Compare your logistic regression model with one feature and an intercept term with linear regression model with same features. In what sense they are similar? In what sense they are different? What are your takeaways from current training and test accuracy? How do you want to further improve your logistic regression model on classification without adding more features?**"]},{"cell_type":"markdown","metadata":{"id":"E4UwKlVIHOTs"},"source":["## Optimize your Simple Logistic Regression Model: One Feature with an Intercept Term\n","\n","In this section, we'll perform two ways to find optimal parameter of logistic regression model: gradient descent and hyperparameter tuning with cross validation."]},{"cell_type":"markdown","metadata":{"id":"FAfLms_CIHYE"},"source":["### Gradient Descent in Logistic Regression Model\n","\n","Last week, you are introduced to gradient descent algorithm, and implement stochastic gradient descent on some dataset. Now you will apply the same thought process on logistic regression. The model parameter we want to optimize is `vec_lr` in $R^2$. The loss function we want to minimize is cross entropy loss as defined in the previous part. Here we will walk through and apply gradient descent algorithm on logistic regression model, and find the minimized loss."]},{"cell_type":"markdown","metadata":{"id":"RKZX3t7VMupc"},"source":["\n","These are common variations of gradient descent algorithms that we mentioned in the course note. $\\alpha$ is step size. $s_i$ is predicted probability of data point $i$.\n","\n","*   Batch gradient descent:\n","$$ \\theta^{(t+1)} = \\theta^{(t)} + \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^{n} { (y_i - s_i) X_i } $$\n","*   Stochastic gradient descent:\n","$$ \\theta^{(t+1)} = \\theta^{(t)} + \\alpha \\cdot (y_i - s_i)X_i $$\n","*   Mini-batch gradient descent:\n","$$ \\theta^{(t+1)} = \\theta^{(t)} + \\alpha \\cdot \\frac{1}{|B|} \\sum_{i \\in B} {(y_i - s_i)X_i} $$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h4VkSwCrVWGd"},"source":["#### Batch Gradient Descent\n","\n","**Question: Implement batch gradient descent on training set, with respect to cross entropy loss function. For simplicity, you will perform in total 1000 iterations, with step size 0.001 defined for you. You may want to first change pandas dataframe of `X_simple_train` to a numpy array to better work with matrix vector multiplication.**"]},{"cell_type":"code","metadata":{"id":"sHf1vUdGPPgR"},"source":["# due to time constraint, we will only implement 10000 iterations\n","num_iter = 10000\n","\n","# set step size to be as small as 0.001\n","alpha = 0.1\n","\n","# for simplicity, first transofmr training set features to numpy array\n","X_simple_train_grad = X_simple_train[['mean radius', 'intercept']].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0sOrHQ-Kq7j"},"source":["# TODO: Implement batch gradient descent on logistic regression model\n","\n","# initialize with random value [0, 0]\n","vec_lr_batch = ...\n","\n","# array to save cross entropy loss throughout process\n","loss_batch = ...\n","\n","# implement batch gradient descent\n","for i in range(num_iter):\n","\n","  # compute loss at current iteration\n","  ...\n","\n","  # initiate gradient update vector\n","  ...\n","\n","  # perform gradient update\n","  ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vRKQ39WVvgi"},"source":["**Question: Plot your cross entropy loss on training set with respect to number of iterations using batch gradient descent. You should see that the loss is consistently decreasing monotonely. Then report the final cross entropy loss value that batch gradient descent converges to.**"]},{"cell_type":"code","metadata":{"id":"6xufM18TPkbG"},"source":["# TODO: visulize change in cross entropy loss with respect to number of iterations\n","\n","plt.figure(figsize=(12, 10))\n","\n","x_axis = ...\n","...\n","\n","plt.xlabel('number of iterations')\n","plt.ylabel('cross entropy loss')\n","plt.title('Cross Entropy Loss on Training Set vs Number of Iterations in Batch Gradient Descent');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9w2I4sNUPEn"},"source":["# TODO: Report final cross entropy loss on training set that batch gradient descent converges to\n","\n","print('Final cross entropy loss on training set with batch gradient descent:', ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qplKbWENckI1"},"source":["**Question: Use the optimal model parameter found by batch gradient descent to compute predicted probability given by logistic regression model. Visualize the decision boundary, i.e. shape of sigmoid function, compared to true label value.**"]},{"cell_type":"code","metadata":{"id":"RB1Fg3cQgyIA"},"source":["# final logistic regression model parameter by batch gradient descent\n","vec_lr_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5u_jv2n_mfmf"},"source":["# TODO: predicted probability of logistic regression model on training set\n","# HINT: `y_pred_probs_simple_train` should be a column vector in same length as number of training samples\n","\n","...\n","batch_probs = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyfNdDVZnK_I"},"source":["# visualize logistic regression predictions\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use scatterplot to visualize true label value of diagnosis, same as what you did in the previous part\n","#       use lineplot to visulize predicted value given by batch logistic regression model\n","\n","...\n","...\n","plt.title('Logistic Regression Prediction on Training Set: Batch Gradient Descent');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9CG8oK3oHs4"},"source":["**Question: Now compute training and test accuracy of batch gradient descent solution to logistic regression model.**"]},{"cell_type":"code","metadata":{"id":"IdErfPfyoctZ"},"source":["# TODO: compute predicted labels of batch logisitc regression model\n","y_pred_label_batch = ...\n","\n","# TODO: Compute training accuracy:\n","print('Training accuracy of Batch Logistic Regression model:', \n","      ...)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_OPlWKBpBds"},"source":["# TODO: compute predicted labels of batch logisitc regression model\n","...\n","batch_test_probs = ...\n","\n","y_pred_test_label_batch = ...\n","\n","# TODO: Compute test accuracy:\n","print('Test accuracy of Batch Logistic Regression model:', \n","      ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yt_WfJ89WRlb"},"source":["#### Stochastic Gradient Descent\n","\n","**Question: Implement stochastic gradient descent on training set, with respect to cross entropy loss function. Use same set of parameters as in the previous part.**"]},{"cell_type":"code","metadata":{"id":"QGvIXS15WnA5"},"source":["# TODO: Implement batch gradient descent on logistic regression model\n","\n","# initialize with random value [0, 0]\n","vec_lr_stochastic = ...\n","\n","# array to save cross entropy loss throughout process\n","loss_stochastic = ...\n","\n","# implement batch gradient descent\n","for i in range(num_iter):\n","\n","  # compute loss at current iteration\n","  ...\n","\n","  # initiate gradient update vector\n","  ...\n","  \n","  # perform gradient update\n","  ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rn4iAKtyXUwA"},"source":["# TODO: visulize change in cross entropy loss with respect to number of iterations\n","\n","plt.figure(figsize=(12, 10))\n","\n","x_axis = ...\n","...\n","\n","plt.xlabel('number of iterations')\n","plt.ylabel('cross entropy loss')\n","plt.title('Cross Entropy Loss on Training Set vs Number of Iterations in Stochastic Gradient Descent');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcMQRChSXsKz"},"source":["# TODO: Report final cross entropy loss on training set that stochastic gradient descent converges to\n","\n","print('Final cross entropy loss on training set with stochastic gradient descent:', ...)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5t8V2KwLniM9"},"source":["# final logistic regression model parameter by stochastic gradient descent\n","vec_lr_stochastic"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQNtVw9Hn0OJ"},"source":["# TODO: predicted probability of logistic regression model on training set\n","# HINT: `y_pred_probs_simple_train` should be a column vector in same length as number of training samples\n","\n","...\n","stochastic_probs = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"20AEkGcdn95O"},"source":["# visualize logistic regression predictions\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use scatterplot to visualize true label value of diagnosis, same as what you did in the previous part\n","#       use lineplot to visulize predicted value given by batch logistic regression model\n","...\n","...\n","plt.title('Logistic Regression Prediction on Training Set: Batch Gradient Descent');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E1gJK5QBquzC"},"source":["**Question: Now compute training and test accuracy of stochastic gradient descent solution to logistic regression model.**"]},{"cell_type":"code","metadata":{"id":"ikKZ5sgrqRR3"},"source":["# TODO: compute predicted labels of stochastic logisitc regression model\n","y_pred_label_stochastic = ...\n","\n","# TODO: Compute training accuracy:\n","print('Training accuracy of Stochastic Logistic Regression model:', \n","      ...)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kETXOtUFqRR4"},"source":["# TODO: compute predicted labels of stochastic logisitc regression model\n","...\n","stochastic_test_probs = ...\n","\n","y_pred_test_label_stochastic = ...\n","\n","# TODO: Compute test accuracy:\n","print('Test accuracy of Stochastic Logistic Regression model:', \n","      ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYSJjCO-YMaP"},"source":["#### Mini-Batch Gradient Descent\n","\n","**Question: Implement mini-batch gradient descent on training set, with respect to cross entropy loss function. Use same set of parameters as in the previous part. Use same set of parameter as train test split, we will sample 20% of training points as batch size to compute gradient update.**"]},{"cell_type":"code","metadata":{"id":"Do49Pp2AYUut"},"source":["# hyperparameter batch size\n","B = int(0.2 * X_simple_train_grad.shape[0])\n","B"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9PidG2uYs37"},"source":["# TODO: Implement batch gradient descent on logistic regression model\n","\n","# initialize with random value [0, 0]\n","vec_lr_mini_batch = ...\n","\n","# array to save cross entropy loss throughout process\n","loss_mini_batch = ...\n","\n","# implement batch gradient descent\n","for i in range(num_iter):\n","\n","  # compute loss at current iteration\n","  ...\n","\n","  # initiate gradient update vector\n","  ...\n","\n","  # initiate gradient update vector\n","  ...\n","\n","  # sample a random subset of training points with batch size B\n","  ...\n","  \n","  # perform gradient update\n","  ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hdsqLGwZu-6"},"source":["# TODO: visulize change in cross entropy loss with respect to number of iterations\n","\n","plt.figure(figsize=(12, 10))\n","\n","x_axis = ...\n","...\n","\n","plt.xlabel('number of iterations')\n","plt.ylabel('cross entropy loss')\n","plt.title('Cross Entropy Loss on Training Set vs Number of Iterations in Mini-Batch Gradient Descent');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NRNyeZ2IaAzd"},"source":["# TODO: Report final cross entropy loss on training set that mini-batch gradient descent converges to\n","\n","print('Final cross entropy loss on training set with mini-batch gradient descent:', ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYZrzaLhq7Vp"},"source":["**Question: Use the optimal model parameter found by mini batch gradient descent to compute predicted probability given by logistic regression model. Visualize the decision boundary, i.e. shape of sigmoid function, compared to true label value.**"]},{"cell_type":"code","metadata":{"id":"yNq-9m5Mq7Vp"},"source":["# final logistic regression model parameter by batch gradient descent\n","vec_lr_mini_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rRojrnLq7Vq"},"source":["# TODO: predicted probability of logistic regression model on training set\n","# HINT: `y_pred_probs_simple_train` should be a column vector in same length as number of training samples\n","...\n","mini_batch_probs = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Asvkhzy0q7Vq"},"source":["# visualize logistic regression predictions\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use scatterplot to visualize true label value of diagnosis, same as what you did in the previous part\n","#       use lineplot to visulize predicted value given by batch logistic regression model\n","...\n","...\n","plt.title('Logistic Regression Prediction on Training Set: Mini-Batch Gradient Descent');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TTulxAQQnrgQ"},"source":["**Question: Use the optimal model parameter found by stochastic gradient descent to compute predicted probability given by logistic regression model. Visualize the decision boundary, i.e. shape of sigmoid function, compared to true label value.**"]},{"cell_type":"code","metadata":{"id":"ywjCYhsMq__i"},"source":["# TODO: compute predicted labels of stochastic logisitc regression model\n","y_pred_label_mini_batch = ...\n","\n","# TODO: Compute training accuracy:\n","print('Training accuracy of Mini-Batch Logistic Regression model:', \n","      ...)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FnS3sSppq__r"},"source":["# TODO: compute predicted labels of stochastic logisitc regression model\n","...\n","mini_batch_test_probs = ...\n","\n","y_pred_test_label_mini_batch = ...\n","\n","# TODO: Compute test accuracy:\n","print('Test accuracy of Mini-Batch Logistic Regression model:', \n","      ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rEsN--q-sDes"},"source":["**Question: Compare your optimal logistic greression model given by 3 types of gradient descent, with initial one you choose without optimize parameters. Comment on your observations.**"]},{"cell_type":"markdown","metadata":{"id":"xkvwcUnttQ0z"},"source":["### Hyperparameter Tuning: Threshold Value in Prediction\n","\n","We are interested in whether changing threshold value in prediction rule will improve our logistic regression model performance. For simplicity, we will use model parameter approximnately same as the result given by gradient descent algorithm."]},{"cell_type":"code","metadata":{"id":"aM6GM8huta8r"},"source":["# Define model parameter for hyperparameter tuning.\n","vec_lr_tune = np.array([-1, 15]).reshape(2, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F4efhjPFt-LH"},"source":["**Question: Compute training accuracy of logistic regression model, with different threshold value between 0 and 1. To help you get started, recall currently we use threshold value of 0.5. Find out the best hyperparameter threshold value, using cross validation.**"]},{"cell_type":"code","metadata":{"id":"imVMwcrkt8Hf"},"source":["# TODO: train validation set split\n","\n","X_tune_train, X_tune_val, y_tune_train, y_tune_val = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKxknaTlvdDL"},"source":["# TODO: set candidate values for threshold\n","thresholds = ...\n","\n","# TODO: Compute tune predictions on training and validation set\n","tune_train_scores = ...\n","tune_train_pred = ...\n","tune_val_scores = ...\n","tune_val_pred = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzPJZMIMwAH7"},"source":["# TODO: Compute training and validation accuracy of differnet threshold value\n","\n","tune_train_accuracy = ...\n","tune_val_accuracy = ...\n","\n","for ...:\n","  tune_train_pred_label = ...\n","  tune_val_pred_label = ...\n","\n","  ...\n","  ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNmk4X_uxzSt"},"source":["**Question: Plot training and validation accuracy with respect to different threshold values.**"]},{"cell_type":"code","metadata":{"id":"klFXZk4vxyKR"},"source":["# visualize hyperparameter tuning accuracy in training set\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use lineplot to visualize training accuracy with respect to different threshold values\n","...\n","\n","plt.xlabel('thresholds')\n","plt.ylabel('training accuracy')\n","plt.title('Logistic Rergession Model Accuracy on Training Set vs Different Threshold Values');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4V9gcLRyDWz"},"source":["# visualize hyperparameter tuning accuracy in training set\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: use lineplot to visualize validation accuracy with respect to different threshold values\n","...\n","\n","plt.xlabel('thresholds')\n","plt.ylabel('validation accuracy')\n","plt.title('Logistic Rergession Model Accuracy on Validation Set vs Different Threshold Values');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZ09BLm7y2KV"},"source":["**Question: Find best threshold value on training set and validation set. Are they the same? Which one you would choose to use in practice?**"]},{"cell_type":"code","metadata":{"id":"OpyiWg10y1sb"},"source":["# TODO: Find best hyperparameter value that gives highest training accuracy\n","print('Best threshold value with highest training accuracy:', ...)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xvo5LU9ZzWBk"},"source":["# TODO: Find best hyperparameter value that gives highest validation accuracy\n","print('Best threshold value with highest validation accuracy:', ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JJHRtQXijDo5"},"source":["## Compare Logistic Regression to Nearst Neighbor Perspective\n","\n","As you visualize decision boundary of logistic regression model on 1 dimensional input, you might find it similar to something we discussed earlier this week."]},{"cell_type":"markdown","metadata":{"id":"Kg-swK3wjTjq"},"source":["We talk about using kernels earlier this week. Kernel function measures similarity between different data points in certain way. In classification problem, we are interested in how features of data points are similar under the condition that they have the same label value.\n","\n","From kernel perspective, prediction on a new test point should be finding the closest data points in feature space, and taking certin weighted average of response values of these closest point. \n","\n","We now want to extend this idea to binary classification problem. Looking at the shape of logistic regression decision boundary above, we propose another way to generate such a similar decision boundary."]},{"cell_type":"markdown","metadata":{"id":"kGfPwffekBhz"},"source":["Given a new test point $x$, suppose we focus on $k$ closest point to $x$ in feature space. We predict the probability of $x$ has label $1$ in binary classification problem with the average of labe values of all $k$ closest point. This is reasonable intuitively, since data points closer to each other in feature space should have same label values. As you should have seen earlier this week, this is exactly how k-nearst neighbor algorithm works in regression and classification problem.\n","\n","**Question: Now we want to implement k-nearst neighbor algorithm in sklearn, visualize its decision boundary, and compare its similarity to logistic regression decison boundary.**"]},{"cell_type":"code","metadata":{"id":"-Pcev7qnkzhy"},"source":["from sklearn.neighbors import NearestNeighbors\n","\n","# TODO: change hyperparameter value k\n","\n","k = ...\n","knn_model = NearestNeighbors(n_neighbors=k)\n","knn_model.fit(X_simple_train[['mean radius', 'intercept']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h46mPU82lTYr"},"source":["# TODO: implement knn predicted probabilities\n","# HINT: first you want to find k closest point from test point in training set\n","#       second you want to compute average of label values of these closest points\n","#       third predictions is given by this mean\n","\n","knn_pred_probs = []\n","for i in range(X_simple_test.shape[0]):\n","  ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Br92_8n8p1__"},"source":["# visualize knn predictions\n","plt.figure(figsize=(10, 6))\n","\n","# TODO: plot true label values and predicted values given by k nearst neighbor\n","\n","plt.title('K Nearst Neighbor Prediction on Test Set: One feature with an Intercept Term');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qymbFczcqMd8"},"source":["**Question: Initialize with hyperparameter $k=5$. Plot your visualization of decision boundaries and report your observations. What does current decision boundary looks like? Do you feel it's a reasonable approach and can generate high test accuracy?**"]},{"cell_type":"markdown","metadata":{"id":"9plda55Pqb6v"},"source":["**Question: Change hyperparameter value $k$, what do you observe when you increase value of $k$? Compare new decision boundary of k-nearst neighbor with large $k$ to logistic regression decision boundary. In what sense they are similar and dissimilar?**"]},{"cell_type":"markdown","metadata":{"id":"rnC-FEQeqwsR"},"source":["**Question: After this trial, how do you want to interpret logistic regression model and power of kernel functions? What would you comment on using kernel and k-nearst neighbor to approximate logistic regression?**"]},{"cell_type":"markdown","metadata":{"id":"-YgDG7ZNfdml"},"source":["## Application of Logistic Regression: Binary Classification in Breast Cancer Dataset"]},{"cell_type":"markdown","metadata":{"id":"JxxPWJyaBEkm"},"source":["In the following part of this coding assignment, you will be looking at breast cancer dataset from UCI machine learning library. This is a well-known dataset in ML field. It includes several information of patients with or without cancer, and we'll use these information to build a binary logistic classifier to predict whether the patient has cancer or not. More details are at UCI machine learning website. "]},{"cell_type":"markdown","metadata":{"id":"CsTLRilRFiH0"},"source":["### Import packages"]},{"cell_type":"markdown","metadata":{"id":"eMKeVJfHCAea"},"source":["Before we get started, we'll need to import packages necessary to our assignment: \n","\n","*   [pandas](https://pandas.pydata.org/pandas-docs/stable/) - a package for performing data analysis and manipulation\n","*   [numpy](https://numpy.org/doc/stable/contents.html) - a package for scientific computing\n","*   [matplotlib](https://matplotlib.org/3.3.2/contents.html) - the standard Python plotting package\n","*   [seaborn](https://seaborn.pydata.org/) - a dataframe-centric visualization package that is built off of matplotlib"]},{"cell_type":"code","metadata":{"id":"-Fgacp63oN5N"},"source":["# import necessary library and setup\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zt32b-Z6Fn0v"},"source":["### Load Dataset and Preliminary Understanding"]},{"cell_type":"markdown","metadata":{"id":"RqAAqtJ_HcBA"},"source":["The Wisconsin breast and cancer dataset is fully understood and analyzed in the ML field. Skit-learn library has a built-int [load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) function that imports this dataset. We no longer need to manually read csv files."]},{"cell_type":"code","metadata":{"id":"AgTX__LkFqG2"},"source":["# import breast cancer dataset from sklearn library\n","from sklearn.datasets import load_breast_cancer\n","dataset = load_breast_cancer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iOhzpbMXF1f_"},"source":["# convert to pandas dataframe \n","features = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n","labels = dataset.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFpWTX56OmiA"},"source":["# number of features and data points\n","print(features.shape)\n","print(labels.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vitysNrdOea8"},"source":["# binary labels\n","list(dataset.target_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hfO9L26KOwNe"},"source":["The entire dataset has 569 samples, there are in total 30 features for each patient sample. The diagnosis of each patient: M = malignant and B = benign corresponds to 0 and 1 values. Sklearn already partially preprocesses the dataset for us, patient ID column that is irrelevant to classfication has been removed.\n","\n","Now let's look at what are feature columns in our dataset."]},{"cell_type":"code","metadata":{"id":"brp8uCsNP74W"},"source":["# peek into features\n","features.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZY8vsO0QLxl"},"source":["# column names\n","print(\"Columns:\", list(features.columns))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZrRKpo4QiWs"},"source":["You can observe that we will deal with different measurements of a single parameter. For example, you will consider \"mean texture\" and \"worst texture\" as two features. You might wonder whether this may lead to high correlation between features. Hopefully you can explore this when you evaluate your model."]},{"cell_type":"markdown","metadata":{"id":"LZTfl_1JURA7"},"source":["For reference, the following are complete attribute information of ten original real valued features from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)):\n","\n","> Ten real-valued features are computed for each cell nucleus:\n","\n","*   radius (mean of distances from center to points on the perimeter)\n","*   texture (standard deviation of gray-scale values)\n","*   perimeter\n","*   area\n","*   smoothness (local variation in radius lengths)\n","*   compactness (perimeter^2 / area - 1.0)\n","*   concavity (severity of concave portions of the contour)\n","*   concave points (number of concave portions of the contour)\n","*   symmetry\n","*   fractal dimension (\"coastline approximation\" - 1)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WDLVinFDQVyW"},"source":["Let's look at whether there is a class imbalance in the dataset. You will examine number of label 0 in the dataset versus number of label 1 in the dataset."]},{"cell_type":"code","metadata":{"id":"KGuc8Y2PQDwR"},"source":["print(\"Number of non-cancer samples:\", sum(labels == 0), \"out of\", len(labels), \"samples in total.\")\n","print(\"Number of cancer samples:\", sum(labels == 1), \"out of\", len(labels), \"samples in total.\")\n","\n","sns.countplot(x = labels)\n","plt.xlabel('diagnosis')\n","plt.title('Number of Benign-0 vs Malignant-1');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CI81jWDQT-f-"},"source":["Fortunately class imbalance is not a significant problem here, since each single class accounts for approximately one half of total number of samples."]},{"cell_type":"markdown","metadata":{"id":"xOB3LTjJVyvl"},"source":["### Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"lLrUUnUuY3Vw"},"source":["Given a dataset to build machine learning model, you need to first deal with missing values in the dataset, if any. Remember there are different ways appriximate missing values: completely delete them, use low rank approximation, or use other statistics such as average of same feature.\n","\n","If you completely delete samples with missing values, this might lead to bias in your model and generalization issues as you might ignore a specific portion in the population. If you use other statistics to replace missing values, be careful with their properties: whether they are sensitive to outliers and what implications they bring etc.\n","\n","You will look at whether we need to deal with missing values in our data set."]},{"cell_type":"code","metadata":{"id":"fL9H72opV4ZM"},"source":["for column in features.columns:\n","  print(\"Column\", column, \"has\", sum(features[column].isnull()), \"missing values.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qJhC2v8DesSP"},"source":["There's no missing values you need to consider in this case. Fortunely, binary labels \"M = malignment\" and \"B = belignment\" are already been one-hot-encoded as numerical labels 1 and 0 correspondingly. So this dataset can be directly used for classification."]},{"cell_type":"markdown","metadata":{"id":"KkHbS1Ks48Tn"},"source":["**Question: Now, construct a pandas dataframe contains means and standard deviation of each column in `features` table. You want to explore whether values of features are in similar range.**"]},{"cell_type":"code","metadata":{"id":"EwUq2DsR3Z2X"},"source":["# TODO: assign mean and sd of each column and make a dataframe containing all values\n","\n","feature_mean_sd = ...\n","feature_mean_sd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlu1V5fy6or2"},"source":["plt.figure(figsize=(20,10))\n","sns.barplot(x='feature', y='average', data=feature_mean_sd)\n","plt.xticks(rotation=45)\n","plt.title('Mean of Features in Wisconsin Breast Cancer Dataset');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ripDn957T_7"},"source":["plt.figure(figsize=(20,10))\n","sns.barplot(x='feature', y='standard deviation', data=feature_mean_sd)\n","plt.xticks(rotation=45)\n","plt.title('Stadard Deviation of Features in Wisconsin Breast Cancer Dataset');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUOUf_u1tRPX"},"source":["**Question: Report your observations ontraining set. What is the range of different features in the training set? What data cleaning technques you may want to use right now?**"]},{"cell_type":"markdown","metadata":{"id":"ok90zoeLttQS"},"source":["**Question: Noramlize dataset `features` loaded from `sklearn` library directly. You can reassign new dataframe to the same variable name without creating a new one.**"]},{"cell_type":"code","metadata":{"id":"Qz5SNb0E8bJO"},"source":["# TODO: normalize dataset `features`\n","# HINT: find mean, std of each column in dataset\n","#       compute standard unit\n","features = ...\n","features.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9z_qT75xfETs"},"source":["### Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"z7bo6Pr8fHxf"},"source":["You should observe original dataset has 10 features, we compute mean, standard deviation, and extreme value of each feature, so we will use 30 features in total to make prediction. You will visualize features from these three perspectives respectively."]},{"cell_type":"code","metadata":{"id":"3o-N6lHtjb5e"},"source":["# split features to groups\n","features_mean = np.array(features.columns[:10])\n","features_error = np.array(features.columns[10:20])\n","features_worst = np.array(features.columns[20:31])\n","\n","# add label column to features\n","features['diagnosis'] = labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slGfqEnPiodG"},"source":["#### Data Visualization of Mean Features"]},{"cell_type":"markdown","metadata":{"id":"ABG0lyW1nJPw"},"source":["We want to observe distribution of all mean feature values, seperated by breast cancer diagnosis labels. We want to learn whether certain feature values are correlated to diagnosis of breast cancer.\n","\n","**Question: Visualize 3 set of features respectively. Specifically, for each set of features, draw a [boxplot](https://matplotlib.org/3.3.2/api/_as_gen/matplotlib.pyplot.boxplot.html) revealing distribution of feature values, separated by diagnosis label values $\\{0, 1\\}$. Then draw a [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) revealing correlation between different features. You should remember that highly correlated features doesn't help improve performance of model.**\n","\n","**Then based on boxplot, briefly describe distribution of values of features in different groups in terms of different label values.**"]},{"cell_type":"code","metadata":{"id":"sUt-PjiniXKh"},"source":["# boxplot of distribution of mean festures, separated by breast cancer binary labels\n","temp = pd.concat([features['diagnosis'], features[features_mean]], axis=1)\n","temp = pd.melt(temp, id_vars=\"diagnosis\", var_name=\"features\", value_name='value')\n","plt.figure(figsize=(12,10))\n","\n","# TODO: boxplot\n","\n","plt.xticks(rotation=45)\n","plt.title(\"Distribution of Mean Feature Values\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02mHdBOinbbx"},"source":["*Solution: You shold observe that among all mean feature values, except \"mean fractal dimension\" has similar distribution among \"M\" and \"B\" labels, all other mean feature distributions have visible differences.*"]},{"cell_type":"markdown","metadata":{"id":"jX6o4VBzoiU5"},"source":["Then you want to look at whether there are high correlation between differnet features in the same group."]},{"cell_type":"code","metadata":{"id":"mxwZtUABnsox"},"source":["# heatmap of correlation between mean features\n","plt.figure(figsize=(10,10))\n","\n","# TODO: heatmap\n","\n","plt.title('Correlation between Mean Feature Values');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aTFrFN2zovEd"},"source":["#### Data Visualization of Error Features"]},{"cell_type":"markdown","metadata":{"id":"z0lWmizfo9Ex"},"source":["We want to observe distribution of all error feature values, seperated by breast cancer diagnosis labels. We want to learn whether certain feature values are correlated to diagnosis of breast cancer."]},{"cell_type":"code","metadata":{"id":"awEc-4xzpKMj"},"source":["# boxplot of distribution of error festures, separated by breast cancer binary labels\n","temp = pd.concat([features['diagnosis'], features[features_error]], axis=1)\n","temp = pd.melt(temp, id_vars=\"diagnosis\", var_name=\"features\", value_name='value')\n","plt.figure(figsize=(12,10))\n","\n","# TODO: boxplot\n","\n","plt.xticks(rotation=45)\n","plt.title(\"Distribution of Error Feature Values\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xw2N_9tpScz"},"source":["*Obviously, distribution of error features has higher variance. There are more outliers in values of variance than means. Except for \"texture error\", all other error features have visible difference in distribution relative to \"M\" and \"B\" labels.*"]},{"cell_type":"code","metadata":{"id":"2dC18CFopwwO"},"source":["# heatmap of correlation between error features\n","plt.figure(figsize=(10,10))\n","\n","# TODO: heatmap\n","\n","plt.title('Correlation between Error Feature Values');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nr_4F6inqPNQ"},"source":["#### Data Visualization of Worst Features"]},{"cell_type":"markdown","metadata":{"id":"dDSHTWc-qWtm"},"source":["We want to observe distribution of all error feature values, seperated by breast cancer diagnosis labels. We want to learn whether certain feature values are correlated to diagnosis of breast cancer."]},{"cell_type":"code","metadata":{"id":"c-j41gmgqcFj"},"source":["# boxplot of distribution of worst festures, separated by breast cancer binary labels\n","temp = pd.concat([features['diagnosis'], features[features_worst]], axis=1)\n","temp = pd.melt(temp, id_vars=\"diagnosis\", var_name=\"features\", value_name='value')\n","plt.figure(figsize=(12,10))\n","\n","# TODO: boxplot\n","\n","plt.xticks(rotation=45)\n","plt.title(\"Distribution of Worst Feature Values\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wjDVW_QOqiLW"},"source":["*Solution: Fortunately, all worst features have distinct distribution for samples in \"M\" and \"B\" labels. This concludes that they are all valid features to be used in prediction.*"]},{"cell_type":"code","metadata":{"id":"IdDsClpfq2SR"},"source":["# heatmap of correlation between worst features\n","plt.figure(figsize=(10,10))\n","\n","# TODO: heatmap\n","\n","plt.title('Correlation between Worst Feature Values');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nZoizJHlrpLf"},"source":["### Logistic Regression Model "]},{"cell_type":"code","metadata":{"id":"j0-HKysYrxnP"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Akf8P4JEupNy"},"source":["**Question: Set up features and observed labels in training set. Recall we already normalize `features` matrix, so no need to redo standardization. Use train test split with ratio of 80% - 20%.**"]},{"cell_type":"code","metadata":{"id":"F9jLwVUTs1hJ"},"source":["# set up X and y matrix\n","features = features.drop('diagnosis', axis=1)\n","X = features\n","y = labels\n","\n","# TODO: train-test split\n","X_train, X_test, y_train, y_test = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FK0s3iLHdoI_"},"source":["**Question: Now fit your training data logistic regression model imported from sklearn library, you may find documentations and examples [in this link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) helpful.**"]},{"cell_type":"code","metadata":{"id":"2A_H4F7XvHtZ"},"source":["# TODO: fit logistic regression model\n","model = ...\n","..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Byb0slaXw4bF"},"source":["You should be able to know that sklearn solves optimal logistic regression weights using gridient descent in loss function, which you cannot implement here. \n","\n","**Question: Now let's simply evaluate our model, using training and test accuracy. Feel free to use builtin functions in sklearn.**"]},{"cell_type":"code","metadata":{"id":"0V8WPwj2vuVw"},"source":["# TODO: training accuracy of our model\n","train_accuracy = ...\n","print(\"Training accuracy of logistic regression classifier:\", train_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jG8PmYK-wPF9"},"source":["# TODO: test accuracy of our model\n","test_accuracy = ...\n","print(\"Test accuracy of logistic regression classifier:\", test_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aD7PKE8XxHVX"},"source":["It's obvious for you to conclude that our logistic regression classifier does a pretty good job on both training and test set, with accuracy over 95% in general. But remember we talked about there are different perspectives to evaluate a machine learning model in classification problem."]},{"cell_type":"markdown","metadata":{"id":"TKWAWgnqwwkF"},"source":["### Evaluation of Binary Classifier"]},{"cell_type":"markdown","metadata":{"id":"q2cuk7fIxkgz"},"source":["#### Confusion Matrix\n","\n","You should be familiari with the role of confusion matrix in classification problem. \n","\n","**Question: Use sklearn library to report confusion matrix of your logistic regression classfier and report your observations. A heatmap might be a good choice here. We already import [metrics](https://scikit-learn.org/0.16/modules/classes.html#module-sklearn.metrics) from sklearn for you.**"]},{"cell_type":"code","metadata":{"id":"COUAxIhuyB_K"},"source":["from sklearn import metrics\n","\n","# TODO: find test confusion matrix\n","test_predictions = ...\n","cm = ...\n","cm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wyskA0rnyif7"},"source":["# visualization of confusion matrix\n","plt.figure(figsize=(8,6))\n","sns.heatmap(data=cm, annot=True)\n","plt.title(\"Confusion Matrix of Logistic Classifier on Breast Cancer Test set\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1VUeFDDHy_ag"},"source":["There's only 5 points misclassified among test set. Half of them coming from either 0 or 1 label. We can only say the classifier performs consistently among two types."]},{"cell_type":"markdown","metadata":{"id":"LIqjEDld0A1r"},"source":["#### Precision, Recall, TPR, and FNR\n","\n","**Question: Based on confusion matrix above, compute FP, TN, FN, TP, precision, recall, TPR, and FPR of our classifier.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5gFqoRiQe8tF"},"source":["*Solution:*\n","\n","*Based on confusion matrix, test predictions are breaked down into following categories:*\n","\n","*   *False Positives (FP): 2*\n","*   *True Negatives (TN): 35*\n","*   *False Negatives (FN): 3*\n","*   *True Positives (TP): 74*"]},{"cell_type":"markdown","metadata":{"id":"hRJYb7hR0g_S"},"source":["*Evaluate value of precision, recall, TPR, and FNR:*\n","\n","*   *precision = $\\frac {TP} {TP + FP} $ = $\\frac {74} {74 + 2}$ = $0.97368421$*\n","*   *recall = $\\frac {TP} {TP + FN} $ = $\\frac {74} {74 + 3}$ = $0.96103896$*\n","*   *TPR = recall = $0.96103896$*\n","*   *FPR = $\\frac {FP} {FP + TN} $ = $\\frac {2} {2 + 35}$ = 0.054054*\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"owCCnMlk7lbH"},"source":["**Question: Now can can plot precision vs. recall with respect to each threshold. This is a similar metrics to ROC curve that we talked about before. You may find `predict_proba()` function of logistic regression in sklearn useful.**"]},{"cell_type":"code","metadata":{"id":"HC4egFbG6wwR"},"source":["# get precision, recall, threshold\n","from sklearn.metrics import precision_recall_curve\n","\n","# TODO: get predicted probability of LR, get precision and recall\n","test_pred_probs = ...\n","test_precision, test_recall, thresholds = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSIfwz_X7JD9"},"source":["# plot precision-recall curve\n","plt.figure(figsize=(12,10))\n","\n","# TODO: use lineplot for precision-recall curve\n","#       make sure to add xlabel and ylabel\n","\n","plt.xlabel('Test Recall')\n","plt.ylabel('Test Precision')\n","plt.title('Precision-Recall Curve of Logistic Regression Classifier on Breast Cancer Test Set');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mTxtR_-2v_1"},"source":["#### ROC Curve and AUC-ROC\n","\n","ROC curve is an important metric to evaluate a classification model. Plot ROC curve of logistic regression model, and report whether it's good or not based on your observations.\n","\n","**Question: Plot ROC curve of logistic regression model on test set.** \n","\n","To get started, you will need to use sklearn metrics library to evalute. Remember logistic regression classifier in sklearn has a function `predict_proba()` that might be useful. You need to use probabilistic interpretation of logistic classifier: logistic regression predicts probability of a test point to have label 1. so you only cares about predicted probability of label 1."]},{"cell_type":"code","metadata":{"id":"wkbVlxGu2-Xr"},"source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCE95G_H3TCa"},"source":["# TODO: get ROC and AUC-ROC probability and score\n","test_pred_probs = ...\n","test_auc = ...\n","print(\"AUC of ROC score in test set:\", test_auc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9O9MqFPa4V9I"},"source":["# plot ROC curve\n","test_fpr, test_tpr, thresholds = roc_curve(y_test, test_pred_probs)\n","plt.figure(figsize=(12,10))\n","\n","# TODO: use lineplot for ROC curve\n","#       make sure to add xlabel and ylabel\n","\n","plt.title('ROC Curve of Logistic Regression Classifier on Breast Cancer Test Set');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PmVeHgKe77ih"},"source":["**Question: Comment on similarity and difference between ROC curve and precision-recall curve of our classifier. What does this say about performance of our logistic regression binary classifier?**"]},{"cell_type":"markdown","metadata":{"id":"NXHGdAavvLpT"},"source":["## Stochastic Gradient Descent\n","\n","Now you want to use gradient descent to iteratively approximate optimal $\\hat{\\theta}$ by minimizing cross entropy loss function. Sklearn provides SGDClassifier model that implements stochastic gradient descent on logistic regression model, by minimizing cross entropy loss function."]},{"cell_type":"code","metadata":{"id":"yveN1FeNvzJG"},"source":["from sklearn.linear_model import SGDClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6o-yhZnrg58F"},"source":["**Question: Fit training set into [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) model. You may want to use custom parameter when you initiate the model, so that it performs classification task instead of regression task.**"]},{"cell_type":"code","metadata":{"id":"PsmI5WVDv58C"},"source":["# TODO: construct stochastic gradient descent learning algorithm\n","sgd_model = ...\n","..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8FEVu56yhNuf"},"source":["**Question: Reprt training accuracy and test accuracy of logistic regression model given by stochastic gradient descent.**"]},{"cell_type":"code","metadata":{"id":"923flHzXxCXN"},"source":["# TODO: training accuracy of sgd model\n","sgd_train_accuracy = ...\n","print(\"Training accuracy of stochastic gradient descent classifier:\", sgd_train_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oslkl2cQxS5E"},"source":["# TODO: test accuracy of sgd model\n","sgd_test_accuracy = ...\n","print(\"Test accuracy of stochastic gradient descent classifier:\", sgd_test_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7UzeKXRVhYsH"},"source":["**Question: Print and visualize confusion matrix of stochastic gradient descent model.**"]},{"cell_type":"code","metadata":{"id":"hMD3Isv0xjMR"},"source":["# TODO: print and visualize confusion matrix\n","sgd_test_predictions = ...\n","sgd_cm = ...\n","sgd_cm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bx923RoCxrbg"},"source":["# visualization of confusion matrix\n","plt.figure(figsize=(8,6))\n","sns.heatmap(data=sgd_cm, annot=True)\n","plt.title(\"Confusion Matrix of Stochastic GD Logistic Classifier on Breast Cancer Test set\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y4_9oVyChjI0"},"source":["**Question: Find precision and recall values at different thresholds by SGD on test set. Plot precision vs recall curve as you did for LR model.**"]},{"cell_type":"code","metadata":{"id":"y0V8t5Tkx1ZZ"},"source":["# TODO: find test predicted probabilities (NOT LABEL VALUES)\n","#       find precision, recall at different thresholds\n","sgd_test_pred_probs = ...\n","sgd_test_precision, sgd_test_recall, thresholds = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHS8yEJdx8EW"},"source":["# plot precision-recall curve\n","plt.figure(figsize=(12,10))\n","\n","# TODO: use lineplot for precision vs recall curve\n","#       make sure to add xlabel and ylabel\n","\n","plt.title('Precision-Recall Curve of SGD Logistic Classifier on Breast Cancer Test Set');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jyv6attbyVsC"},"source":["# TODO: get ROC and AUC-ROC probability and score\n","sgd_test_pred_probs = ...\n","sgd_test_auc = ...\n","print(\"AUC of ROC score in test set:\", test_auc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVTZ_wgLyeOS"},"source":["# plot ROC curve\n","\n","# TODO: get FPR, TPR at different thresholds\n","sgd_test_fpr, sgd_test_tpr, thresholds = ...\n","\n","plt.figure(figsize=(12,10))\n","\n","# TODO: use lineplot for ROC curve\n","#       make sure to add xlabel and ylabel\n","\n","plt.title('ROC Curve of SGD Logistic Classifier on Breast Cancer Test Set');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAFMbnjQylpV"},"source":["**Question: Observe performance of logistic regression model trained by stochastic gradient descent. How is it different from sklearn logistic regression model? Does stochastic gradient descent algorithm give us higher accuracy? How about value in confusion matrix and AUC-ROC? How would you connect to properties ot SGD we discussed last week?**"]},{"cell_type":"markdown","metadata":{"id":"6N1GaZaQttsF"},"source":["**Bonus: Change step size $\\eta$ in stochastic gradient descent model you fit. Observe change in convergence. What's your takeaway if you want to use SGD to solve for optimal parameter in LR model?**"]},{"cell_type":"markdown","metadata":{"id":"MSFHHXvsdLeY"},"source":["Congratulations on finishing this assignment! Hope you get intuition of how does logistic regression model exactly solve binary classification problem. Hope you understand why we introduce it as the most fundamental model in clssification problem, and as a generalized linear model. You should get hands on experiences about how to use sklearn to fit a logistic regression model, and how to evaluate from different perspectives."]}]}